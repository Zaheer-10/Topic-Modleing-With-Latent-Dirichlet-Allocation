{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ae4dd-4630-4146-9e82-d23e38fac8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b10d28-b035-4743-97de-73ae0409944b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297ee78-0d6f-423f-8726-28032f15e6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2459d68f-07d7-4802-b8c0-9506165e4bab",
   "metadata": {},
   "source": [
    "# Topic Modeling with Gensim (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9dc68-cb32-4c81-87df-4e0b18bd98ba",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc46a59-b2b3-470c-a9a1-604bbdaab748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c690a1-936e-4c5f-91ba-7a498207c848",
   "metadata": {},
   "source": [
    "### Prepare Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f724b718-758b-4b06-bd2f-5871c0e2cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d062775-0409-4005-a930-6f521b67a44a",
   "metadata": {},
   "source": [
    " ### Import Newsgroups Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b0519-78d3-4863-a570-22e4b6b79f04",
   "metadata": {},
   "source": [
    "#### I will be using the 20-Newsgroups dataset. This version of the dataset contains about 11k newsgroups posts from 20 different topics. This is available as newsgroups.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03638ceb-68db-419e-b458-bf9a1167fd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
      " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
      " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
      " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
      " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
      " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target   \n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7  \\\n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e06de-ce53-4f2b-83cd-b213ccdb25c2",
   "metadata": {},
   "source": [
    "###  Remove emails and newline characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3d121-1e4c-4814-8cc8-03a3e70b4e45",
   "metadata": {},
   "source": [
    "#### As you can see there are many emails, newline and extra spaces that is quite distracting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f947f2ab-2023-4822-84f7-a0182525d4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
      " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
      " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition, the front bumper was separate from the rest of the body. This is '\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
      " 'production, where this car is made, history, or whatever info you have on '\n",
      " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
      " 'your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e6bae2-4432-47bd-ab28-327b8c4f512d",
   "metadata": {},
   "source": [
    "#### After removing the emails and extra spaces, the text still looks messy. It is not ready for the LDA to consume. You need to break down each sentence into a list of words through tokenization, while clearing up all the messy text in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b8053-8d66-4d7d-8979-59e329ae0070",
   "metadata": {},
   "source": [
    "### Tokenize words and Clean-up text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20d59592-0123-4667-9fb6-c1369991aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e30ff2-86c2-41c8-a929-1665935178a5",
   "metadata": {},
   "source": [
    "### Creating Bigram and Trigram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff069fd-feed-4c79-b6e8-14d5ccedd797",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Bigrams are two words frequently occurring together in the document. \n",
    "#### Trigrams are 3 words frequently occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bbb45a-df72-4b40-b78f-139d62759503",
   "metadata": {},
   "source": [
    "### # Build the bigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb027288-74bf-4935-9c3a-026e8535d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e28e2-a5de-4496-bf6c-9367fe0c1ab7",
   "metadata": {},
   "source": [
    "### # Faster way to get a sentence clubbed as a trigram/bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76e225ca-12c6-4751-963f-d401567f6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33f26c3c-c253-48eb-b766-0e921dd79194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d21bd-1135-4f3c-b665-4156ccb7a883",
   "metadata": {},
   "source": [
    "### Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4c4af-d48c-46e7-9b61-8f09caacce2d",
   "metadata": {},
   "source": [
    "#### The bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e24f400-81eb-4d43-b0cf-2c9576f773c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0a467-bbec-47ee-a5eb-d704d2dbe0bd",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec9837c0-d5f0-4eb5-947a-ae83308c22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c073e-0261-49c6-a910-82fb7438841d",
   "metadata": {},
   "source": [
    "#### Form Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "666d6115-b520-46a2-a6e1-57304157c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4273e246-4d33-4fb6-9b96-d9fb4c043e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\" , disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa56a76-56cb-45bb-b3d5-6fc1dbe22d49",
   "metadata": {},
   "source": [
    "### Do lemmatization keeping only noun, adj, vb, adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "490e7c62-9c61-41f8-9e5d-12c882f8a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5ff9140-11db-4d60-b9b1-3a2144b04d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['s', 'thing', 'car', 'nntp_poste', 'host', 'rac_wam', 'university', 'park', 'line', 'wonder', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fcbcc-13ba-4aac-84b9-af9f447f5398",
   "metadata": {},
   "source": [
    "### Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854aea79-2fa3-4e9e-b48a-91a746f91ac4",
   "metadata": {},
   "source": [
    "#### The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1e28447-3a3e-4372-b8e6-7c585af6fec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 5), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3da0110d-545a-46ee-9fc5-c895cfcc2113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'addition'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8527aec-fde7-4209-adaa-42ff6061e399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('addition', 1),\n",
       "  ('body', 1),\n",
       "  ('bring', 1),\n",
       "  ('call', 1),\n",
       "  ('car', 5),\n",
       "  ('day', 1),\n",
       "  ('door', 2),\n",
       "  ('early', 1),\n",
       "  ('engine', 1),\n",
       "  ('enlighten', 1),\n",
       "  ('funky', 1),\n",
       "  ('history', 1),\n",
       "  ('host', 1),\n",
       "  ('info', 1),\n",
       "  ('know', 1),\n",
       "  ('late', 1),\n",
       "  ('lerxst', 1),\n",
       "  ('line', 1),\n",
       "  ('look', 2),\n",
       "  ('mail', 1),\n",
       "  ('make', 1),\n",
       "  ('model', 1),\n",
       "  ('name', 1),\n",
       "  ('neighborhood', 1),\n",
       "  ('nntp_poste', 1),\n",
       "  ('park', 1),\n",
       "  ('production', 1),\n",
       "  ('rac_wam', 1),\n",
       "  ('really', 1),\n",
       "  ('rest', 1),\n",
       "  ('s', 1),\n",
       "  ('see', 1),\n",
       "  ('separate', 1),\n",
       "  ('small', 1),\n",
       "  ('spec', 1),\n",
       "  ('sport', 1),\n",
       "  ('thank', 1),\n",
       "  ('thing', 1),\n",
       "  ('university', 1),\n",
       "  ('wonder', 1),\n",
       "  ('year', 1)]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b57796-85e6-4356-9620-0b7970693057",
   "metadata": {},
   "source": [
    "### Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c70cc113-6990-42f2-8cd5-8d9b9bb06bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde6251-f1a6-4120-b7e9-d75c1f6c70ec",
   "metadata": {},
   "source": [
    "<h5 id=\"chunksize-is-the-number-of-documents-to-be-used-in-each-training-chunk.-update_every-determines-how-often-the-model-parameters-should-be-updated-and-passes-is-the-total-number-of-training-passes.\">'chunksize ' is the number of documents to be used in each training 'chunk. update_every' determines how often the model parameters should be updated and</h5>\n",
    "<h5>'passes' is the total number of training passes.</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2a93f-42a2-4c80-b4e0-75cf0a81873e",
   "metadata": {},
   "source": [
    "###  View the topics in LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a7f72-189d-4eb3-83b3-a6e9886400bd",
   "metadata": {},
   "source": [
    "#### The above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "200c190c-ee69-4bee-8a25-dba6fe2991be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.021*\"research\" + 0.019*\"information\" + 0.019*\"high\" + 0.019*\"report\" + '\n",
      "  '0.018*\"player\" + 0.016*\"service\" + 0.015*\"rate\" + 0.014*\"design\" + '\n",
      "  '0.013*\"season\" + 0.012*\"low\"'),\n",
      " (1,\n",
      "  '0.077*\"team\" + 0.072*\"game\" + 0.053*\"play\" + 0.050*\"faith\" + 0.049*\"win\" + '\n",
      "  '0.031*\"belief\" + 0.025*\"atheist\" + 0.025*\"year\" + 0.018*\"wing\" + '\n",
      "  '0.018*\"score\"'),\n",
      " (2,\n",
      "  '0.106*\"space\" + 0.029*\"notice\" + 0.029*\"launch\" + 0.026*\"earth\" + '\n",
      "  '0.024*\"mission\" + 0.024*\"orbit\" + 0.023*\"external\" + 0.020*\"vehicle\" + '\n",
      "  '0.019*\"satellite\" + 0.019*\"door\"'),\n",
      " (3,\n",
      "  '0.022*\"say\" + 0.019*\"people\" + 0.017*\"reason\" + 0.017*\"believe\" + '\n",
      "  '0.015*\"evidence\" + 0.014*\"mean\" + 0.012*\"point\" + 0.012*\"question\" + '\n",
      "  '0.011*\"many\" + 0.010*\"claim\"'),\n",
      " (4,\n",
      "  '0.078*\"book\" + 0.044*\"science\" + 0.042*\"reference\" + 0.036*\"pin\" + '\n",
      "  '0.032*\"section\" + 0.025*\"faq\" + 0.024*\"author\" + 0.023*\"copy\" + '\n",
      "  '0.023*\"reality\" + 0.022*\"internal\"'),\n",
      " (5,\n",
      "  '0.065*\"cost\" + 0.059*\"model\" + 0.039*\"character\" + 0.036*\"picture\" + '\n",
      "  '0.036*\"format\" + 0.032*\"quality\" + 0.032*\"associate\" + 0.028*\"handle\" + '\n",
      "  '0.023*\"hole\" + 0.023*\"gift\"'),\n",
      " (6,\n",
      "  '0.032*\"system\" + 0.028*\"use\" + 0.024*\"program\" + 0.023*\"file\" + '\n",
      "  '0.018*\"card\" + 0.016*\"run\" + 0.014*\"software\" + 0.014*\"bit\" + '\n",
      "  '0.013*\"machine\" + 0.013*\"problem\"'),\n",
      " (7,\n",
      "  '0.092*\"moral\" + 0.056*\"property\" + 0.045*\"serial\" + 0.036*\"lock\" + '\n",
      "  '0.022*\"positively\" + 0.021*\"intent\" + 0.018*\"alarm\" + 0.012*\"converter\" + '\n",
      "  '0.011*\"unnecessary\" + 0.007*\"provision\"'),\n",
      " (8,\n",
      "  '0.249*\"window\" + 0.057*\"monitor\" + 0.055*\"normal\" + 0.041*\"do\" + '\n",
      "  '0.032*\"font\" + 0.023*\"left\" + 0.020*\"widget\" + 0.019*\"please_respond\" + '\n",
      "  '0.017*\"environment\" + 0.017*\"trivial\"'),\n",
      " (9,\n",
      "  '0.061*\"child\" + 0.028*\"church\" + 0.027*\"woman\" + 0.025*\"armenian\" + '\n",
      "  '0.022*\"authority\" + 0.020*\"community\" + 0.019*\"greek\" + 0.017*\"period\" + '\n",
      "  '0.017*\"turk\" + 0.016*\"soldier\"'),\n",
      " (10,\n",
      "  '0.765*\"ax\" + 0.035*\"physical\" + 0.024*\"graphic\" + 0.014*\"direct\" + '\n",
      "  '0.011*\"convert\" + 0.006*\"daughter\" + 0.006*\"capture\" + 0.005*\"human_being\" '\n",
      "  '+ 0.004*\"split\" + 0.003*\"accomplish\"'),\n",
      " (11,\n",
      "  '0.130*\"line\" + 0.076*\"organization\" + 0.074*\"write\" + 0.063*\"article\" + '\n",
      "  '0.056*\"nntp_poste\" + 0.050*\"host\" + 0.029*\"reply\" + 0.024*\"thank\" + '\n",
      "  '0.018*\"university\" + 0.013*\"post\"'),\n",
      " (12,\n",
      "  '0.072*\"plane\" + 0.030*\"hi\" + 0.021*\"subscription\" + 0.020*\"steve\" + '\n",
      "  '0.015*\"divide\" + 0.011*\"evolve\" + 0.010*\"intersection\" + 0.010*\"rip\" + '\n",
      "  '0.008*\"upcoming\" + 0.007*\"script\"'),\n",
      " (13,\n",
      "  '0.031*\"people\" + 0.028*\"state\" + 0.018*\"gun\" + 0.017*\"government\" + '\n",
      "  '0.017*\"law\" + 0.016*\"right\" + 0.015*\"kill\" + 0.013*\"death\" + 0.011*\"live\" + '\n",
      "  '0.011*\"force\"'),\n",
      " (14,\n",
      "  '0.141*\"drug\" + 0.029*\"film\" + 0.026*\"movie\" + 0.025*\"stereo\" + '\n",
      "  '0.024*\"japanese\" + 0.022*\"deficit\" + 0.020*\"plot\" + 0.014*\"mad\" + '\n",
      "  '0.009*\"harley\" + 0.007*\"deck\"'),\n",
      " (15,\n",
      "  '0.061*\"box\" + 0.050*\"club\" + 0.041*\"modem\" + 0.041*\"status\" + '\n",
      "  '0.030*\"primary\" + 0.029*\"routine\" + 0.029*\"spec\" + 0.026*\"sufficient\" + '\n",
      "  '0.023*\"public_access\" + 0.023*\"automatically\"'),\n",
      " (16,\n",
      "  '0.152*\"drive\" + 0.091*\"car\" + 0.036*\"bike\" + 0.024*\"engine\" + 0.023*\"nhl\" + '\n",
      "  '0.022*\"ride\" + 0.018*\"road\" + 0.017*\"weight\" + 0.016*\"mile\" + '\n",
      "  '0.015*\"ground\"'),\n",
      " (17,\n",
      "  '0.113*\"patient\" + 0.060*\"disease\" + 0.054*\"scientific\" + '\n",
      "  '0.050*\"computer_science\" + 0.043*\"animal\" + 0.041*\"health\" + '\n",
      "  '0.040*\"treatment\" + 0.037*\"medical\" + 0.033*\"dog\" + 0.030*\"study\"'),\n",
      " (18,\n",
      "  '0.023*\"get\" + 0.018*\"go\" + 0.015*\"good\" + 0.015*\"time\" + 0.015*\"know\" + '\n",
      "  '0.014*\"make\" + 0.013*\"well\" + 0.013*\"think\" + 0.012*\"see\" + 0.010*\"take\"'),\n",
      " (19,\n",
      "  '0.106*\"key\" + 0.043*\"test\" + 0.032*\"public\" + 0.031*\"encryption\" + '\n",
      "  '0.028*\"security\" + 0.028*\"server\" + 0.022*\"clipper\" + 0.021*\"chip\" + '\n",
      "  '0.018*\"secure\" + 0.018*\"message\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d72bce-ef70-43b7-bd3d-05b3a1db8877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff42cea3-5c39-4844-9528-ab677a88aa0d",
   "metadata": {},
   "source": [
    "### Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f47a7699-b853-43a8-9d1c-756d59959a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -13.32461339253533\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "\n",
    "## a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee5b85-48aa-463b-b0ec-233c6ee9c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50abd8-f542-40e5-98de-12064f249551",
   "metadata": {},
   "source": [
    "### Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d48c6-30b3-4afb-88d0-1e55d3d38c8a",
   "metadata": {},
   "source": [
    "##### Now that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package’s interactive chart and is designed to work well with jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09135ebc-fc9d-4b11-965c-d6321d5cc616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbbf00-ed95-4252-987b-df1b58878a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ddc1f-69c2-494f-ab08-f8bac8173d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1561ce8-6a60-432d-a6e0-8ef722181bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e3ab27-6e47-4d93-98fe-9047c6293dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbc40e-bebd-4e32-a8a7-503990847cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bbdd7-0035-4e13-8819-7cc4a95b573d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07251e50-8dda-4c2f-a1db-3e93dbfc689b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300a68a-f090-4406-bb25-3e6f74465f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4eef2-e23c-4700-9e67-43e2d87cb411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec4214-e75c-47e3-9cd6-7f75b61c39fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47832584-1d25-4997-aea3-9c7afb8e262f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
